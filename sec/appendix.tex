\newpage
\appendix

\section{Kalman Filter} \label{app:kalman}

Using the definitions from ~\cite{welch1995introduction}, Kalman Filter (KF) estimates the state $x_n \in \mathbb{R}^N$ of a linear system driven by the below equations, given the measurement $z_n \in \mathbb{R}^M$:
\begin{align}
x_n = A x_{n - 1} +  w_{n - 1} \\
z_n = J x_n + v_n
\end{align}

where $w_n$ is process noise, $v_n$ is measurement noise with $p(w) \sim \mathcal{N}(0, Q)$ and $p(v) \sim \mathcal{N}(0, R)$. 
The matrix $A$ maps the state at the previous time step to the state at current time step. The matrix $J$ maps the state $x_n$ to the measurement $z_n$.

We assume that while  there is no new measurements, an estimate of the current hand parameters are the previous known parameters up to Gaussian noise, that is $x_n = x_{n-1} + w_{n-1}$. We also assume for now that depth sensor noise is normally distributed, which actually is not the case. Thus, the states of our system can be approximated as following:
\begin{align}
x_n = x_{n - 1} + w_{n - 1} \\
z_n = J x_n + v_n
\end{align}

Define $\hat{x}_n^0$ to be initial state estimate at step $n$ and $\hat{x}_n$ to be a state estimate after updating on the measurement $z_n$.

Then the covariance before and after the measurement is then 
\begin{align}
P_n^0=\mathbb{E}[(x_n - \hat{x}_n^0)^T(x_n - \hat{x}_n^0)]\\
P_n =\mathbb{E}[(x_n - \hat{x}_n)^T(x_n - \hat{x}_n)]
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!h] 
\centering
\caption{Time and measurement update for KF with $A$ equal to identity} 
\resizebox{0.41\textwidth}{!}{
\begin{tabular}{|c|c|}
\hline
Time Update & Measurement Update \\
\hline

$\hat{x}_n^0 = \hat{x}_{n - 1}$ & 
$K_n = P_n^0 J^T (J P_n^0 J^T + R)^{-1}$ \\

$P_n^0 = P_{n - 1} + Q$ &
$\hat{x}_n = \hat{x}_n^0 + K_n (z_n - J \hat{x}_n^0)$ \\

 & 
$P_n = (I - K_n J) P_n^0$ \\

\hline
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Special case from the Section  \ref{sec:combining}}
Let us consider the special case when the measurement $z_n = x_n^*$ is in the same space at the estimated state $\hat{x}_n$, thus $J$ is equal to identity.

\begin{align*}
\hat{x}_n = \hat{x}_n^0 + P_n^0  (P_n^0 + R)^{-1}(z_n - \hat{x}_n^0) = \\
= (P_n^0 + R)(P_n^0 + R)^{-1}\hat{x}_n^0 + P_n^0  (P_n^0 + R)^{-1}(z_n - \hat{x}_n^0) = \\
= R(P_n^0 + R)^{-1}\hat{x}_n^0 + P_n^0  (P_n^0 + R)^{-1}z_n 
 \end{align*}

\begin{align*}
P_n = ((P_n^0 + R) (P_n^0 + R)^{-1} - P_n^0  (P_n^0 + R)^{-1}) P_n^0\\
P_n = R (P_n^0 + R)^{-1} P_n^0
\end{align*}

The expressions for $\hat{x}_n$ and $P_n$ coincide with Equations \ref{eq:combining} for product of two Gaussians  with $z_n = x_n^*$, $P_n^0 = \hat{\Sigma}_{n - 1}$ and $R = \Sigma_{n}$.

\section{Extended Kalman Filter}

Following ~\cite{welch1995introduction}, Extended Kalman Filter (EKF) estimates the state $x_n$  of a non-linear system given the measurement $z_n$.
\begin{align}
x_n = \tilde{F}(x_{n - 1},  w_{n - 1}) \\
z_n = F(x_n, v_n)
\end{align}

The function $F(\cdot)$ that maps the state $x_n$ to the measurement $z_n$ applies shape and pose parameters to the hand model and computes the closest model points to sensor data points. 
The function $\tilde{F}(\cdot)$ relates the state at the previous time step to the state at current time step, in our case $\tilde{F}(\cdot)$ is an identity mapping. Thus, $\frac{ \partial \tilde{F}_{[i]}}{ \partial x_{[j]}}(\hat{x}_{n - 1}, 0) \equiv I$, 
$\frac{ \partial \tilde{F}_{[i]}}{ \partial w_{[j]}}(\hat{x}_{n - 1}, 0) \equiv I$ and $\frac{ \partial F_{[i]}}{ \partial v_{[j]}}(\hat{x}_n^0, 0) \equiv I$, where $I$ is an identity matrix of the corresponding size.
\begin{align}
x_n = x_{n - 1} + w_{n - 1} \\
z_n = F(x_n) + v_n 
\end{align}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!h] 
\centering
\caption{Time and measurement update for EKF with $\tilde{F}(\cdot)$ being an identity mapping} 
\resizebox{0.42\textwidth}{!}{
\begin{tabular}{|c|c|}
\hline
Time Update & Measurement Update \\
\hline

$\hat{x}_n^0=\hat{x}_{n-1}$ &
$K_n = P_n^0 J_n^T(J_n P_n^0 J_n^T + R)^{-1}$ \\

$P_n^0 = P_{n-1} + Q$ &
$\hat{x}_n = \hat{x}_n^0 + K_n(z_n - F_n)$ \\

 &
$P_n = (I - K_n J_n)P_n^0$ \\

\hline
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

with $F_n = F(\hat{x}_n^0)$ and ${J_n}_{[i, j]} = \frac{ \partial F_{[i]}}{ \partial x_{[j]}}(\hat{x}_n^0)$.

\section{Iterated Extended Kalman Filter}

In our case the measurement function $F(\cdot)$ is nonlinear such that the Levenberg-Marquardt algorithm
$\hat{x}_n^{i + 1} = (J(\hat{x}_n^i) ^T J(\hat{x}_n^i) + \lambda I)^{-1} J(\hat{x}_n^i)^T F(\hat{x}_n^i)$ takes several iterations to converge. Thus we perform measurement update in several steps each time linearizing measurement function  $F(\cdot)$ around the updated value $\hat{x}_n^i$. We use the equations of Iterated Extended Kalman Filter (IEKF) presented in ~\cite{havlik2015performance}. The measurement update equations respectively for EKF and IEKF are presented in Table \ref{tab:ekf-iekf}. The time update equations remain the same as before.

\begin{table}[!h] 
\centering
\caption{EKF vs IEKF measurement update \label{tab:ekf-iekf}}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|l|l|}
\hline
Extended Kalman Filter & Iterated Extended Kalman Filter \\
\hline

  &
for $i = 1...$ \\
  
$F_n = F(\hat{x}_n^0)$ &
$\:\:\: F_n^\textit{\color{accent}$i$} = F( \hat{x}_n^\textit{\color{accent}$i$})$ \\

${J_n}_{[u, v]} = \frac{ \partial F_{[u]}}{ \partial x_{[v]}}(\hat{x}_n^0)$ & 
$\:\:\: {J_n}_{[u, v]}^\textit{\color{accent}$i$} = \frac{ \partial F_{[u]}^\textit{\color{accent}$i$}}{ \partial x_{[v]}}(\hat{x}_n^\textit{\color{accent}$i$})$\\

$K_n = P_n^0 J_n^T(J_n P_n^0 J_n^T + R)^{-1}$ &
$\:\:\: K_n^\textit{\color{accent}$i$} = P_n^0 {J^\textit{\color{accent}$i$}_n}^T(J_n^\textit{\color{accent}$i$} P_n^0 {J^\textit{\color{accent}$i$}_n}^T + R)^{-1}$ \\

$\hat{x}_n = \hat{x}_n^0 + K_n(z_n - F_n)$ &
$\:\:\: \hat{x}_n^\textit{\color{accent}$i + 1$} = \hat{x}_n^0 + K_n^\textit{\color{accent}$i$}(z_n - F_n^\textit{\color{accent}$i$} - $\\

 &$\:\:\:\:\:\:\:\:\:\:\:\: - \textit{\color{accent} $J_n^i(\hat{x}_n^0 - \hat{x}_n^i)$})$ \\

 &
end \\
 
 & $\hat{x}_n = \hat{x}_n^\textit{\color{accent}$i$}$ \\
 
$P_n = (I - K_n J_n)P_n^0$ &
$P_n = (I - K_n^\textit{\color{accent}$i$} J_n^\textit{\color{accent}$i$})P_n^0$ \\

\hline
\end{tabular}
}
\end{table}

\section{Equivalence of IEIF and Optimization \ref{tab:iekf-like}} \label{app:ieif-lm}

To show that the optimizing the objective  \ref{tab:iekf-like} with LM algorithm is equivalent to IEKF, we first rewrite IEKF in information form as shown in \cite{anderson1979optimal}. The two algorithms presented in Table \ref{tab:ekf-eif} yield the same result.

\begin{table}[!h] 
\centering
\caption{EKF vs EIF measurement update \label{tab:ekf-eif}} 
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|l|l|}
\hline
Extended Kalman Filter & Extended Information Filter \\
\hline
 
 & $H_n = \left(P_n\right)^{-1}$ \\

$K_n = P_n^0 J_n^T(J_n P_n^0 J_n^T + R)^{-1}$ &
$H_n = H_n^0 + J_n^T R^{-1} J_n$  \\

$\hat{x}_n = \hat{x}_n^0 + K_n(z_n - F_n)$ &
$K_n = {H_n}^{-1} J_n^T R^{-1}$\\
 
$P_n = (I - K_n J_n)P_n^0$ &
$\hat{x}_n = \hat{x}_n^0 + K_n (z_n - F_n)$ \\

\hline
\end{tabular}
}
\end{table}

Table \ref{tab:ieif} presents the measurement update for Iterated Extended Information Filter, which is equivalent to Iterated Extended Kalman Filter. We assume that measurement noise is i.i.d. for each sensor sample, thus $R = \operatorname{diag}(r)$ with $r$ a positive scalar.

\begin{table}[!h] 
\centering
\caption{Iterated Extended Information Filter} \label{tab:ieif}
\begin{tabular}{|l|}
\hline

for $i = 1...$ \\

$\:\:\:\:\: H_n^\textit{\color{accent}$i$} = \frac{1}{r} (r H_n^0 +{J_n^\textit{\color{accent}$i$}}^T J_n^\textit{\color{accent}$i$})$  \\
  
$\:\:\:\:\: K_n^\textit{\color{accent}$i$} =\frac{r}{r} {H_n^\textit{\color{accent}$i$}}^{-1} {J_n^\textit{\color{accent}$i$}}^T$  \\

$\:\:\:\:\: \hat{x}_n^\textit{\color{accent}$i + 1$} = \hat{x}_n^0 + K_n^\textit{\color{accent}$i$}(z_n - F_n^\textit{\color{accent}$i$} - \textit{\color{accent} $J_n^i(\hat{x}_n^0 - \hat{x}_n^i)$})$ \\

end \\
 
$\hat{x}_n = \hat{x}_n^\textit{\color{accent}$i$}$ \\

\hline
\end{tabular}
\end{table}

 The LM update in Optimization \ref{tab:lm} has the following form: 
 
 \begin{equation}
 x_n^{i + 1}= x_n^{i} - (\bar{J}_n^T \bar{J}_n)^{-1} \bar{J}_n^T \bar{F}_n 
\end{equation}
 
 with $\bar{J}_n$ and $\bar{F}_n$ expanded below, $F_n^i = F(x_n^i)$, 
 $J_n^i = \frac{\partial F}{\partial x_n}(x_n^i)$, the measurement $z_n = d_n$ and $\hat{\Sigma}_n^{-1} = r H_{n - 1}$.
 
 \begin{align*}
x_n^{i + 1}= x_n^{i} - \left(
\left[
	\begin{array}{cc}
		- J_n^i \\
		\left(r H_{n - 1}\right)^{1/2} \\
	\end{array}
\right]^T 
\left[
	\begin{array}{c}
		- J_n^i \\
		\left(r H_{n - 1}\right)^{1/2} \\
	\end{array}
\right]
\right)^{-1} \times\\
\times \left[
	\begin{array}{cc}
		- J_n^i \\
		\left(r H_{n - 1}\right)^{1/2} \\
	\end{array}
\right]^T 
\left[
	\begin{array}{c}
		z_n - F_n^i \\
		\left(r H_{n - 1}\right)^{1/2}(x_n^i - \hat{x}_{n - 1}) \\
	\end{array}
\right] = \\
= x_n^{i} - \underbrace{\left({J_n^i}^T J_n^i + r H_{n - 1}\right)^{-1}}_{A} \times \\
\times \left( - {J_n^i}^T(z_n - F_n^i) + r H_{n - 1}(x_n^i - \hat{x}_{n - 1})\right)  = \\
= x_n^i +\underbrace{A {J_n^i}^T(z_n - F_n^i)}_B - A r H_{n - 1}(x_n^i - \hat{x}_{n - 1})  = \\
= \hat{x}_{n - 1} + B  - A r H_{n - 1}(x_n^i - \hat{x}_{n - 1}) + A A^{-1}( x_n^i - \hat{x}_{n - 1}) = \\
= \hat{x}_{n - 1} + B + A(- r H_{n - 1} + {J_n^i}^T J_n^i + r H_{n - 1})(x_n^i - \hat{x}_{n - 1}) = \\
= \hat{x}_{n - 1} + B + A {J_n^i}^T J_n^i (x_n^i - \hat{x}_{n - 1}) = \\
= \hat{x}_{n - 1} + \underbrace{\left({J_n^i}^T J_n^i + r H_{n - 1}\right)^{-1} {J_n^i}^T}_{K_n^i} \times \\
\times (z_n - F_n^i - J_n^i (\hat{x}_{n - 1} - x_n^i)),
 \end{align*}
 
 which is equivalent to the measurement update of IEIF from Table \ref{tab:ieif}, with $H_n^0 = H_{n-1}$ and $x_n^0 = \hat{x}_{n - 1}$.
 
