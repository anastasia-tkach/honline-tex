\section{Evaluation}
To evaluate the technical validity of our approach we:
(\Sec{evaldataset}) verify its effectiveness on by applying it to a new calibration dataset acquired through commodity depth cameras;
(\Sec{analysis}) corroborate the formulation of our optimization on a synthetic 3D dataset;
(\Sec{evalsynth}) analyze its robustness through randomly perturbing the algorithm initialization; 
(\Sec{evalstar}) attest how our method achieves state-of-the-art performance on publicly available datasets.

\todo{The advantage of \OfflineSoft{} over \OfflineHard{} can be clearly observed in \Figure{evalnyu}, where the former achieves a performance comparable to the one of the (overfitting) intra-frame optimization. 
\TODO{in practice we subsample and take one every twenty frames so that the problem can fit into memory}}



\subsection{Synthetic dataset: robustness -- \Fig{synthetic}}
\label{sec:evalsynth}
We empirically evaluate the \emph{robustness} of the algorithm by analyzing its convergence properties. 
For synthetic data the ground truth shape parameters $\bar\beta$ are readily available, and the sphere-mesh model $\mathcal{M}(\theta,\beta)$ is animated in time with the $\bar\theta_n$ parameters of the complex motions in the \texttt{Handy/Teaser} dataset~\cite{tkach2016sphere}.
The model $\mathcal{M}(\theta,\beta)$ is rendered with the camera intrinsics producing a depth map $\mathcal{D}(\theta,\beta)$. We employ two metrics, one measuring average ground-truth residuals, the other measuring the average length of data-to-model ICP correspondences (i.e. part of the tracking energy):
% 
\begin{align}
E_{\beta} &= \tfrac{1}{|M|} \sum_{m \in M} \left| \beta_{[m]} - \bar\beta_{[m]} \right|
\\
E_\text{d2m}^n &= \tfrac{1}{|d_n|} \sum_{\mathbf{p}_j \in d_n} \| \mathbf{p}_j - \Pi_{\mathcal{D}(\theta,\beta)}(\mathbf{p}_j) \|_2
\label{eq:metrics}
\end{align}
% 
Where $\Pi$ is an operator computing the closest-point projection of point in the sensor's point cloud, onto the point-cloud associated with the synthetic depth-map $\mathcal{D}(\theta,\beta)$; see~\cite{tkach2016sphere}. For ground-truth comparisons, note how we selected only a subset \todo{$M=\{1,2,3,4\}$} of shape parameters; see \Figure{handmodel}. This is necessary as sphere-centres on the palm can move without affecting the tracking energy -- a null-space of our optimization.
% 
The tracking algorithm is initialized in the first frame by $\bar\theta_0$, while initializations for the user-personalized models are drawn from the uniform distribution {\small $\mathcal{U}(\bar\beta, \sigma)$}.
We limit ourselves to ten samples per value of $\sigma$, as each sample requires the re-execution of our algorithm on an entire sequence. Nonetheless, we empirically noticed that increasing the number of samples did not alter our observations.
% \AT{I still REALLY don't like this... It is not statistically significant. We'd run at least 50-100 samples...}
In \Figure{synthetic}, we visualize our two metrics as we vary the intensity of the perturbation. Our perturbation analysis is performed over \todo{2h30s} of synthetic footage, thus in our \VideoSynth{}, we only report a \emph{random selection} of tracked sequences.
% 

\subsection{Synthetic dataset: formulation analysis -- \Fig{interreal}}
\label{sec:analysis}
In \Figure{interreal}, we report an experiment analogous to that of \Figure{inter} but on a full 3D sequence. For example, consider the {\small \todo{$\beta_{[?]}$}} plot, where we report the runtime estimates for the length of the middle-finger's middle-phalanx (to avoid confusion with frame number $n$, the \todo{{\scriptsize $[?]$}} subscript is implied). Congruously to the discussion in \Section{technical}, the phalanx length can be estimated only in those frames where the finger is bent; see callout renderings.
Notice how the intra-frame estimate {\small $\star\beta_n$} often oscillates away from the ground truth: 
such incorrect estimates should be discarded while accumulating information. To achieve this, our algorithm estimates the solution uncertainty {\small $\Sigma^*_n$}, and accordingly updates the online cumulative estimates~{\small $\hat\beta_n$}~and~{\small $\hat\Sigma_n$}.

\subsection{Calibration dataset: \texttt{GuessWho?} -- \Fig{realtrack}}
\label{sec:evaldataset}
We stress-tested our system by qualitative evaluating our calibration technique with data acquired from \emph{twelve} different users performing in front of an Intel RealSense SR300 camera (time-of-flight depth sensor). 
Snapshot of the twelve calibration sequences are reported in \Figure{realtrack}, but the calibration procedure can be better appreciated by watching our \VideoQualitative{}.
While ground truth information is not available, these datasets can be evaluated through the use of empirical metrics; e.g. $E_\text{d2m}$ and $E_\text{m2d}$ from \cite{tkach2016sphere}, or the golden-energy from~\cite{taylor2016joint}.
Hence, to foster research in this domain, we release these calibration sequences to the community as the ``\texttt{GuessWho?}'' dataset -- future works will be able to generate identical synthetic clones of human hands (metrics~$\approx 0$), and thus enable biometric authentication applications.

\input{fig/synthetic/item}
\subsection{State-of-the-art on $\texttt{NYU}$ dataset --
\Fig{evalnyu}}
\label{sec:evalstar}
Although several marker-based \emph{marker-based} datasets are available, such as \cite{qian2014realtime}, \cite{sharp2015accurate} and \cite{yuan2017bighand}, state-of-the-art \emph{generative} methods have focused on the \texttt{NYU} \cite{tompson2014real} and \texttt{Handy} \cite{tkach2016sphere} datasets for quantitative evaluation. These datasets allow us to compare our method and its variants to a number of other algorithms including: the PSO tracker by~\cite{sharp2015accurate}, the calibration methods by~\cite{khamis2015learning} and \cite{tan2016fits}, the subdivision tracker of~\cite{taylor2016joint}, the cylindrical tracker by~\cite{htrack}, the sphere-mesh tracker by~\cite{tkach2016sphere}, as well as the Gaussian tracker in~\cite{sridhar2015fast}, and finally discriminative methods such as~\cite{tompson2014real}, \cite{tang2015opening} and \cite{oberweger2015hands}. 
% 
On the \texttt{NYU} dataset, to properly compare to~\cite{taylor2016joint}, we evaluate the metrics on the first 2440 frames (user \#1). Our algorithm achieves very competitive tracking performance while being the \emph{first} capable to calibrate the user-personalized tracking model \emph{online}, rather than in an offline calibration session like~\cite{taylor2016joint}. 
% 
Notice how the best performance is achieved by either (1) the intra-frame optimization, where per-frame \emph{overfitting} takes place, or (2) by offline calibration techniques such as \OfflineSoft{} or \cite{taylor2016joint} that have a clear advantage (i.e. infinite memory) over streaming algorithms.

\subsection{State-of-the-art on $\texttt{Handy}$ dataset --
\Fig{evalhandy}}


\input{fig/evalnyu/item}
\input{fig/evalhandy/item}


