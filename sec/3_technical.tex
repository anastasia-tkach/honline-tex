\section{Technical}

\AN{The shape prior should only be used in the first frame, because afterwards it will be incorporated inside of Kalman prior}.

\input{fig/uncertainty/item}
\subsection{Independent Solve}
\label{sec:independent}
% 
Denote the vector of all hand parameters as $x_n = [\theta_n; \beta_n]$ and the segmented hand point cloud in vectorized form as $d_n$. Given $d_n$ and initialization of hand pose from previous frame $x_n^0 = x_{n - 1}^*$ we find an independent estimate of $x_n^*$ by local Gauss-Newton optimization of the multi-objective energy function:
% 
\begin{equation}
x_n^* = \argmin_{x_n} \sum_{\tau \in \mathcal{T}} E_{\tau}(d_n, x_n) \label{eq:energies}
\end{equation}
% 
Where $\mathcal{T}$ is the following set:
%
\vspace{-.5\parskip}
\begin{equation*}
\setlength{\jot}{0pt}
\begin{aligned}
\text{\textbf{d2m}} & \quad \text{data points are explained by the model} \\ 
\text{\textbf{m2d}} & \quad \text{the model lies in the sensor visual-hull} \\
\text{\textbf{shape-prior}} & \quad \text{model shape should be plausible} \\
\text{\textbf{pose-prior}} & \quad \text{model pose should be plausible} \\
\text{\textbf{limits}} & \quad \text{joint limits should hold} \\
\text{\textbf{collision}} & \quad \text{fingers should not interpenetrate} 
\end{aligned}
\end{equation*}
With the exception of \emph{shape-prior}, \todo{see \Section{shapeprior}}, the energy terms in the objective function are defined in~\cite{tkach2016sphere}.

\subsection{Posterior distribution of parameters after taking a measurement into account} \label{sec:posterior}

All the energy term $E_{\tau}(d_n, x_n)$ can be rewritten in a form $\|I_{\tau} d_n - F_{\tau} (x_n)\|_2^2$ with $I_{d2m}$ an identity matrix, $I_{m2d}$ a permutation(/reduction?) matrix that removes from $d_n$ the entries that are not the closest for any model point, and the remaining $I_{\tau}$ all zeros matrices.
% 
Denote $\left[F_{d2m}^T(x_n), ..., F_{pose}^T(x_n)\right]^T$ as $F(x_n)$ and overload $d_n$ as  $\left[(I_{d2m} d_n)^T, ..., (I_{pose} d_n)^T\right]^T$  for brevity of notation.

Given the hand point cloud at the first frame $d_1$, we find the best-fitting parameters, see Section \ref{sec:independent}.  We omit the index $1$ of $x$ in the below derivation.
 %
\begin{equation}
x^* = \argmin_{x} \underbrace{\log  P(d|x)}_{L(x)} \label{eq:independent}. 
\end{equation}
%
with $P(d|x) = \exp \left( - \tfrac{1}{2}(d - F(x))^T (d - F(x)) \right)$.

Following~\todo{[add reference on Laplace Approximation]}, we expand the log-likelihood of the data $L(x)$ around the optimal solution $x^*$ to the second order.
%
\begin{align}
L(x) \approx L(x^*)   
+ \left. \tfrac{\partial L(x) }{\partial x}\right|_{x^*}  \Delta x 
+ \left. \tfrac{1}{2} \Delta x^T\tfrac{\partial \partial L(x)}{\partial x \partial x} \right|_{x^*} \Delta x 
\end{align}
%
where $\Delta x=x - x^*$ and Jacobian and Hessian are respectively zero and positive definite at our critical point $x^*$:
%
\begin{align*}
\left.\tfrac{\partial L(x)}{\partial x} \right|_{x^*} = - 2 F(x^*)^T 
\left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*} = 0 
\\
\left. \tfrac{\partial \partial L(x)}{\partial x \partial x}  \right|_{x^*}  = 
2 \left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*}^T 
\left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*} + \\
2 F(x^*)^T  \left. \tfrac{\partial \partial F (x^*) }{\partial \partial x} \right|_{x^*}
\approx 2
\left.\tfrac{\partial F}{\partial x}\right|_{x^*}^T  
\left.\tfrac{\partial F}{\partial x}\right|_{x^*}
\succ 0
\end{align*}
% 
Denoting $\left. \tfrac{\partial \partial L(x)}{\partial x \partial x}  \right|_{x^*}  $ as $\Sigma$ and taking an exponential of both sides gives:
%
\begin{equation}
P(d|x) \propto \exp\left(- \tfrac{1}{2}(x - x^*)^T \Sigma^{-1}  (x - x^*) \right)\\
\end{equation}
% 
Thus, after processing the information from the first frame $d_1$, the quadratic approximation of posterior distribution of hand parameters is a normal distribution $\mathcal{N}\left(\hat{x}_1 = x_1^*,  \hat{\Sigma}_1 = \Sigma_1 \right)$.
%

\subsection{Merging independent measurements} 
\label{sec:combining}

Given the second frame $d_2$, we compute the corresponding value of the parameters $x_2^*$ and their variance $\Sigma_2$ the same way as in \ref{sec:posterior}. The posterior distribution of the parameters now has the form:
% 
\begin{equation}
\mathcal{N}(\hat{x}_2, \hat{\Sigma}_2) = \mathcal{N}(\hat{x}_1, \hat{\Sigma}_1) \mathcal{N}(x_2^*, \Sigma_2) \quad\quad
\end{equation}
% 
applying the product of Gaussians rule ~\cite{petersen2008matrix}, we obtain:
% 
\begin{align}
\begin{split}
\hat{x}_2 &= \Sigma_2 (\hat{\Sigma}_1 + \Sigma_2)^{-1} \hat{x}_1 + 
\hat{\Sigma}_1 (\hat{\Sigma}_1 + \Sigma_2)^{-1} x_2^*
\\
\hat{\Sigma}_2 &= \hat{\Sigma}_1 (\hat{\Sigma}_1 + \Sigma_2)^{-1} \Sigma_2
\label{eq:combining}
\end{split}
\end{align}

In Appendix \ref{app:kalman}, we shown how \Equation{combining} is equivalent to Kalman filter update with measurement $x_n^*$ and measurement noise covariance $\Sigma_n$ summarized in \Figure{interframe}.

\AN{This is important, because, according to P. Maybeck \cite{maybeck1979stochastic}, under the assumptions presented in Appendix \ref{app:kalman} ``the Kalman filter can be shown to be the best filter of any conceivable form. It incorporates all information that can be provided to it.''}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht] 
\centering
\caption{KF: Combining independent measurements\label{tab:kf-like}} 
\begin{tabular}{|c|}
\hline
$x_n^* = \argmax_{x_n} \log P(d_n|x_n)$ \\
$\hat{x}_n = \argmax_{x_n} \underbrace{\log \left( P(x_n|x_n^*) P(x_n |\hat{x}_{n - 1}) \right)}_{L(x_n)}$\\
with \\
$P(x_n |x_n^*) = \exp \left( - (x_n - x_n^* )^T \Sigma_n^{-1}(x_n - x_n^* )\right)$ \\
$P(x_n |\hat{x}_{n - 1}) = \exp \left( - (x_n - \hat{x}_{n - 1} )^T \hat{\Sigma}_{n - 1}^{-1} (x_n - \hat{x}_{n - 1} )\right)$ \\	

$\hat{\Sigma}_n^{-1} = \left. \tfrac{\partial \partial L(x_n)}{\partial x_n \partial x_n}\right|_{\hat{x}_n} \approx $ \\
\\
$\left[
	\begin{array}{cc}
		\left(\Sigma_n^{-1}\right)^{1/2} \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right]^T 
\left[
	\begin{array}{c}
		\left(\Sigma_n^{-1}\right)^{1/2} \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right] = \hat{\Sigma}_{n-1}^{-1}  + \Sigma_{n}^{-1}$ \\

\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Regularizing optimization}

Note that the best-fitting parameters $x_n^*$ for the given the single input frame $d_n$ are computed using local LM optimization. Thus, it is crucial that optimization starts in a sensible region of parameters space. The optimization presented in Table \ref{tab:kf-like} does not provided any information about the current estimate of the parameters $\hat{x}_n$ to the independent solve from Section \ref{sec:independent}. One way to remedy this is include the term $P(x_n |\hat{x}_{n - 1})$ directly in the independent solve.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht] 
\centering
\caption{Objective function of IEKF \label{tab:iekf-like}} 
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|c|}

\hline

$\hat{x}_n = \operatorname{argmax}_{x_n} 
\underbrace{\log \left( P(d_n|x_n) P(x_n |\hat{x}_{n - 1}) \right)}_{L(x_n)}$\\ 
with \\
$P(d_n|x_n) = \exp \left( \tfrac{1}{2} - (d_n - F(x_n))^T (d_n - F(x_n)) \right)$ \\
$P(x_n |\hat{x}_{n - 1}) = \exp \left( \tfrac{1}{2} -(x_n - \hat{x}_{n - 1})^T \hat{\Sigma}_{n - 1}^{-1} (x_n - \hat{x}_{n - 1})\right)$ \\
\\
$\hat{\Sigma}_n^{-1} =  \left. \tfrac{\partial \partial L(x_n)}{\partial x_n \partial x_n}\right|_{\hat{x}_n} \approx $ \\
\\
$\left[
	\begin{array}{cc}
		- \left. \tfrac{\partial F(x_n)} {\partial x_n} \right|_{\hat{x}_n} \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right]^T 
\left[
	\begin{array}{c}
		-\left. \tfrac{\partial F(x_n)} {\partial x_n} \right|_{\hat{x}_n} \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right] = $\\
\\
$ = \hat{\Sigma}_{n-1}^{-1} + \left. \tfrac{\partial F(x_n)} {\partial x_n} \right|_{\hat{x}_n}^T
\left. \tfrac{\partial F(x_n)} {\partial x_n} \right|_{\hat{x}_n} = \hat{\Sigma}_{n-1}^{-1}  + \Sigma_{n}^{-1}$	\\
\hline
\end{tabular}
}
\end{table}

In Appendix \ref{app:ieif-lm} we demonstrate that optimizing the objective function presented in Table \ref{tab:iekf-like} using Levenberg-Marquardt algorithm is equivalent to measurement update of Iterated Extended Kalman Filter.

\subsection{Comparing the two optimizations}
The optimization presented in Table \ref{tab:kf-like} will be used as a baseline. It treats the results of the independent solve $x_n^*$ as a measurement. This approach, arguably can be considered to be the simplest way of maintaining an online parameters estimate. 
The optimization shown in Table \ref{tab:iekf-like} represents the entire hand tracking system as an Iterated Extended Kalman Filter. Given the next measurement $d_n$, the estimated value of the parameters $\hat{x}_n$,  is directly output by the hand tracking system.
