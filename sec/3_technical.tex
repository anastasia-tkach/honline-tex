\input{fig/intra/item}

\section{Online model calibration}
\label{sec:technical}
Given the $n$-th input depth frame $\mathcal{D}_n$ we first segment the hand point cloud via a wristband~\cite{htrack}, and represent it in matrix form as $d_n$. Let $x_n = [\theta_n; \beta_n]$ denote the vector of coalesced pose and shape parameters at frame $n$. Our model $\mathcal{M}(\theta_n; \beta)$ is a sphere-mesh tracking model~\cite{tkach2016sphere} where shape is encoded via scalar length parameters $\beta$ instead of sphere positions; see \Figure{handmodel} and \cite{edoardo} in our \textbf{additional material}. To achieve online \emph{joint} calibration and tracking, we introduce  two fundamental components which we refer to as \emph{intra-} and \emph{inter-frame} regression. The former gathers estimates from a single frame (see \Section{independent}), while the latter integrates this knowlege across frames. We first demonstrate how inter-frame regression can be interpreted as a Kalman Filter (KF) (see \Section{split}), highlight its shortcomings, and finally propose a \emph{joint} inter-/intra-frame regression. We show how this approach can be interpreted as an Iterative Extended Kalman Filter (IEKF)~(see \Section{joint}).
\begin{edit}
In \Section{offline}, we conclude by describing two \emph{offline} calibration procedures that will be used as baselines for comparisons to our online calibration solutions.    
\end{edit}

\paragraph{Intra-frame regression}
Given an initialization $x_n^0$ (obtained from discriminative method or from the solution of the previous time-step) and the input data $d_n$, intra-frame regression solves for a locally optimal $x_n^*$ and estimates its  \emph{uncertainty}. That is, this scheme provides a distribution for $x_n$ that is solely based on the knowledge in the depth frame $\mathcal{D}_n$. Estimating uncertainty in the solution is essential for template personalization. As we illustrate in \Figure{intra}, the confidence in regressed \emph{shape} parameters is conditional on the \emph{pose} of the current frame.
%
%Rather than deriving this relationship via hard-coded rules,  we follow the ideas in ~\todo{\cite{?} and derive how this distribution can be approximated by a Gaussian  $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$.} %\AN{How can one derive hard-coded rules?}
% \AN{We use Laplace Appriximation to find a Gaussian distribution $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$ that best approximates the posterior of the parameters optimized for the current frame  $\mathcal{D}_n$.}
\begin{edit}
A fundamental observation is that the parameters of this distribution can be  obtained from a Laplace approximation of the posterior for $\mathcal{D}_n$, and this allows us to \emph{easily} derive uncertainty: as a function of the the Jacobians of the tracking energies. As illustrated in \Figure{covariance}, these certainties are optimally expressed as covariance matrices~$\star{\Sigma}_n$.
\end{edit}
% We also discuss why a covariance matrix $\star{\Sigma}_n$, as opposed to per-measurement variances, is necessary
% Through the example in , it is possible to notice the fundamental role of , as opposed to
\input{fig/inter/item}

\paragraph{Inter-frame regression}
Unlike pose parameters, shape parameters are \emph{persistent} over time: we observe the same user performing in front of the camera. However, sufficient information to estimate certain shape parameters is simply not available in certain frames. For example, by observing a straight finger like the one in \Figure{intra}, it is difficult to estimate the length of a phalanx. Therefore, knowledge must be gathered from a \emph{collection} of frames capturing the user in different poses. Rather than manually picking a few frames in different poses as in~\cite{taylor2016joint}, 
% collecting a persistent model from a set of pre-recorded calibration data \AN{maybe say manually picked poses to make it sound worse, as it actually is} as in~\cite{taylor2016joint},
we approach the problem from an \emph{online}-modeling viewpoint. In what follows, we detail a principled method to temporally integrate parameter estimates in a way that also accounts for their confidence.

\input{fig/covariance/item}

\newpage
\subsection{Intra-frame regression -- $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$}
\label{sec:independent}
\label{sec:intra}
% 
Given the point cloud $d_n$ at frame $n$, and an initialization of the hand pose from the previous frame $x_n^0 = x_{n - 1}^*$ we find an independent estimate of $x_n^*$ by local iterative optimization (a-la Levenberg) of the multi-objective energy function:
% 
\begin{equation}
x_n^* = \argmin_{x_n} \sum_{\tau \in \mathcal{T}} E_{\tau}(d_n, x_n) 
\label{eq:energies}
\end{equation}
% 
Where the terms $\mathcal{T}$ ensure that:
%
% \vspace{-.5\parskip}
\begin{equation*}
\setlength{\jot}{0pt}
\begin{aligned}
\text{\textbf{d2m}} & \quad \text{data points are explained by the model} \\ 
\text{\textbf{m2d}} & \quad \text{model lies in the sensor visual-hull} \\
\text{\textbf{smooth}} & \quad \text{recorded sequence is smooth} \\
\text{\textbf{pose-prior}} & \quad \text{calibrated hand pose is likely} \\
\text{\textbf{shape-prior}} & \quad \text{calibrated hand shape is likely} \\
\text{\textbf{pose-valid}} & \quad \text{semantics: collisions and joint limits} \\
\text{\textbf{shape-valid}} & \quad \text{semantics: finger order and connectivity}
\end{aligned}
\end{equation*}
The energy terms in the objective function are detailed in~\cite{tkach2016sphere} and~\cite{htrack}, with the exception of \emph{shape-prior} and \emph{shape-valid} that are discussed in \Section{shapeprior}.

\input{tab/kf-like}
\paragraph{Laplace approximation}
% \subsection{Posterior distribution of parameters after taking a measurement into account}
\label{sec:posterior}
While the optimization problem in \Equation{energies} estimates pose/shape parameters, information regarding the confidence of the estimate is not directly available. To derive our uncertainties, we start by converting the data terms \emph{d2m} and \emph{m2d} of \Equation{energies} into probabilistic form:
%
%The data terms \emph{d2m} and \emph{m2d} can be written as posterior distributions: \AN{this is not a posterior distribution yes, and why distributions in plural?}
% 
%
% \begin{DRAFT}
% Towards this goal, we rewrite the \emph{d2m} and \emph{m2d} terms in \Eq{energies} as:
% \begin{align}
% E_{\tau}(d_n, x_n) = \|I_{\tau} d_n - F_{\tau} (x_n)\|_2^2,
% \end{align}
% where $I_\tau$ are identity matrices, and $I_{m2d}(i,i)=0$ if the rasterized 2D template pixel $\mathbf{p}_i$ lies within the sensor silhouette image; see \cite{htrack}. We also concatenate the two terms in pair of column vectors:
% \begin{align*}
% F(x_n) = \left[F_{d2m}(x_n); F_{m2d}(x_n)\right]
% \:\:\text{and}\:\:
% d_n = \left[(I_{d2m} d_n); (I_{m2d} d_n) \right],
% \end{align*}
% \end{DRAFT}
% leading to a compact posterior distribution representation:
% \AN{As we discussed last time, could you please remove this part with indicator variables since I am not using all energies anyway? It takes time to understand it and it does not add anything for the value of the paper.}
% 
\begin{align}
P(d_n|x_n) &= \exp \left( - \tfrac{1}{2}(d_n - F(x_n))^T (d_n - F(x_n)) \right)
\label{eq:posterior}
\end{align}
% 
By temporarily omitting the frame index $n$ for conveniency, our problem is rewritten as a maximum likelihood optimization:
%
\begin{align}
x^* &= \argmax_{x} \underbrace{\log  P(d|x)}_{L(x)}
\label{eq:independent}
\end{align}
%
We now perform a second-order Taylor expansion of the log-likelihood of the data $L(x)$ around the \emph{optimal} solution $x^*$:
%
\begin{align}
L(x) \approx \tilde{L}(x) = L(x^*)   
+ \tfrac{\partial L(x^*) }{\partial x}  \Delta x 
+ \tfrac{1}{2} \Delta x^T\tfrac{\partial^2 L(x^*)}{{\partial x}^2} \Delta x + \text{h.o.t.}
\label{eq:taylor}
\end{align}
%
where $\Delta x=x - x^*$, and let {\small $\partial f(\star{x})/\partial x$} indicate the partial derivative of $f(x)$ evaluated at $\star{x}$  with an abuse of notation. We rewrite {\small $\bar{F}(x_n) = d_n - F(x_n)$} for brevity. Note how the Jacobian and the Hessian are respectively zero and positive definite at our optimal point $x^*$ (see \cite[Sec.~10.2]{nocedal2006numerical}):
%
\begin{align}
\tfrac{\partial L(x^*)}{\partial x} &= - \bar{F}(x^*)^T 
\tfrac{\partial \bar{F}(x^*)}{\partial x} = 0 
\label{eq:taylor-jacobian}
\\
\tfrac{\partial^2 L(x^*)}{\partial x^2} 
% = 2 \left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*}^T \left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*} + \\ 2 F(x^*)^T  \left. \tfrac{\partial \partial F (x^*) }{\partial \partial x} \right|_{x^*}
& \approx - \tfrac{\partial \bar{F}(x^*)}{\partial x}^T \tfrac{\partial \bar{F}(x^*)}{\partial x}
\triangleq %< DEFINED AS
-{\star{\Sigma}}^{-1} \prec 0
\label{eq:taylor-hessian}
\end{align}
% 
From \Equation{taylor}, using $\tilde P(d|x) = \exp (\tilde{L}(x))$, we can then derive the \emph{approximated} posterior distribution:
%
\begin{align}
\tilde{P}(d|x) = \exp\left(- \tfrac{1}{2}(x - x^*)^T {\star{\Sigma}}^{-1}  (x - x^*) \right) = \mathcal{N}\left(\star{x}, \star{\Sigma} \right)
\end{align}
%
That is, after processing the information in a frame $d_n$, the sought-after quadratic approximation of posterior distribution of model parameters is a \emph{normal} distribution $\mathcal{N}\left(x_n^*, \star\Sigma_n \right)$.
Its mean $x^*$ is the solution of the (iterative) optimization in \Equation{energies}; its covariance  $\star\Sigma_n$ is computed according to \Equation{taylor-hessian}, that is, from the Hessian of the objective function evaluated at the optimal solution~$x^*$.
%Jacobians of the energy terms $\{E_{\text{d2m}}, E_{\text{m2d}}\} at the optimal solution.$ \AN{for $E_{\text{d2m}}$ Equation 6 is an approximation of the hessian, not jacobian, since E = F^T * F}


\input{tab/iekf-like}
\input{fig/realtrack/item}
\subsection{Split inter-frame regression -- $\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n) | \mathcal{N}(\star{x}_n, \star{\Sigma}_n)$ } 
% \subsection{Merging independent measurements}
\label{sec:split}
Given the probabilistic interpretation in \Section{independent}, a temporal regression over parameters can be obtained by cumulating the posterior distributions over the set of previous frames. This leads to the following pair of inductive update equations:
% 
\begin{align}
\mathcal{N}(\hat{x}_1, \hat{\Sigma}_1) &= \mathcal{N}(\star{x}_1, \star{\Sigma}_1) \\
\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n) &= \mathcal{N}(\hat{x}_{n-1}, \hat{\Sigma}_{n-1}) \mathcal{N}(\star{x}_n, \star{\Sigma}_n)
\end{align}
% 
\begin{edit}
where $\star{x}_n$ refers to estimates from frame~$n$, while $\hat{x}_n$ represents the estimate of the parameters accumulated from the beginning of the sequence till frame $n$.
\end{edit}
% 
By applying the product of Gaussians rule~\cite{petersen2008matrix}:
% 
\begin{align}
\begin{split}
\hat{x}_{n} &= \star\Sigma_{n} (\hat{\Sigma}_{n-1} + \star\Sigma_{n})^{-1} \hat{x}_{n-1} + 
\hat{\Sigma}_{n-1} (\hat{\Sigma}_{n-1} + \star\Sigma_n)^{-1} x_n^*
\\
\hat{\Sigma}_n &= \hat{\Sigma}_{n-1} (\hat{\Sigma}_{n-1} + {\star\Sigma_n})^{-1} \star\Sigma_n = {\star{\Sigma}_n}^{-1} + \hat{\Sigma}_{n-1}^{-1}
\label{eq:combining}
\end{split}
\end{align}
% 
In \Appendix{proof-kalman}, we shown how \Equation{combining} is equivalent to the~Kalman Filter (KF) update equations in \Table{interframe}, with measurement $x_n^*$, and measurement noise covariance $\star\Sigma_n$. This observation is fundamental as, under the same assumptions we make in \Appendix{kalman}, \cite[Pg. 7]{maybeck1979stochastic} noted how a KF is provably \emph{optimal}.
This optimization, which we refer to as \emph{split inter-frame} is arguably the simplest way of achieving an online parameter regression: by treating the results of the independent solve $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$ as the measurements in a KF.

\subsection{Joint inter-frame regression -- $\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n)$}
\label{sec:joint}
% Note that the best-fitting parameters $x_n^*$ for the given the single input frame $d_n$ are computed using local LM optimization. Thus, it is crucial that optimization starts in a sensible region of parameters space.
The optimization in \Tab{interframe} does not provide any information about the current estimate of the parameters $\hat{x}_n$ to the independent solve described in \Section{independent}. This could be problematic, as in this case \Eq{energies} does not leverage any temporal information aside from initialization, while relying on a sufficiently good initialization to compute $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$. 
%
%One potential way to increase the robustness of the algorithm would be to include this information in \Eq{independent}:
% 
%\begin{align}
%x_n^* &= \argmax_{x_n} \log  P(d_n|x_n) P(x_n |\hat{x}_{n - 1}) 
%\label{eq:independenttemp}
%\end{align}
% 
%Instead, 
%\AN{Why do you discuss one potential way that we never use or refer to afterwards?}
%
We propose to coalesce the inter and intra frame optimization resulting in the \emph{joint} inter-frame regression scheme in~\Table{iekf-like}. 
\begin{edit}
The optimization in ~\Table{iekf-like} can be expressed in least-squares form, and embedded in \Equation{energies} through the term:
\end{edit}
% 
\begin{equation}
E_\text{iekf} = \| \hat{\Sigma}^{-1/2}_{n - 1}(x_n - \hat{x}_{n - 1})\|_2^2 
\label{eq:iekflm}   
\end{equation}
%
In \Appendix{ieif-lm} we demonstrate that optimizing the objective in~\Table{iekf-like} with a Levenberg-Marquardt method is equivalent to an Iterated Extended Kalman Filter (IEKF) with measurement update $d_n$. 
\begin{edit}
To the best of our knowledge, this simple relationship between LM and IEKF in \Equation{iekflm} was unknown. In a practical setting, this observation creates a very simple way to encode IEKF-like behavior within existing LM optimization codebases.
\end{edit}


% \AN{Thus in \Appendix{ieif-lm} we have proven that adding a  $E_\text{iekf}$ to a standard LM sum of energies optimization (that is used in our field all the time) turns the entire system into Iterated Extended Kalman Filter. If you already have an LM system, this can be literally coded in hours at most including debug time :). To our knowledge, this trick was not known/used before. }
%Notice how this optimization, representing our online tracking/modeling system, jointly considers the measurement $d_n$ as %well as past estimates $\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n)$.

\input{fig/interreal/item}

\subsection{Shape regularizers}
\label{sec:shapeprior}
Hand shape variation can be explained in a low dimensional space whose fundamental degrees of freedom include variation like uniform scale, palm width, and finger thickness \cite{khamis15learning}. In our paper we follow the ideas presented in~\cite{edoardo}, and build a latent-space encoding hand shape variability through \emph{anisotropic scaling}. By setting $\omega_\text{shape-space}{\ll}1$, this prior acts as a soft regularizer and does not prevent the algorithm from computing a tight fit to the observed user:
% 
\begin{equation}
E_\text{shape-space} = \|\beta - (\bar{\beta} \circ \mathcal{I}\tilde{\beta}) \|^2
\label{eq:shapespace}
\end{equation}
% 
where $\tilde\beta \in \mathbb{R}^3$ is a latent vector encoding relative changes in hand \emph{height}, \emph{width} and sphere \emph{thickness} with respect to the default template $\bar\beta$, while $\mathcal{I}$ is a matrix mapping latent DOFs to the corresponding full-dimensional DOFs $\beta_{[i]}$; see \Figure{handmodel}. 
% 
%While this regularizer effectively sub-spaces our calibration problem, it cannot detect unfeasible shape-space \AN{we are not solving in a subspace and shape prior is totally against the wierd stuff, it just has too small weight to prevent it from happening.}
% \AN{Thanks to our sphere-meshes hand model representation, the shape parameters are already nearly as compact as it gets, thus the last thing we want is to solve our optimization in shape-subspace; hence the small weight on $E_\text{shape-space}$. However, we still need a way to prevent inhumane configurations, }
\begin{edit}
While such prior can produce \emph{likely} hand shapes, unfeasible hand-shape configurations are still possible, such as a finger floating in mid-air, or when the natural order of fingers $\{\text{index},\text{middle},\text{ring},\text{pinky}\}$ has been compromised. 
\end{edit}
We overcome this problem by a set of quadratic \emph{barrier} constraints that are conditionally enabled in the optimization when unfeasible configurations are detected (encoded via $\chi_c(\beta) \in \{ 0,1 \}$):
% 
\begin{equation}
E_\text{shape-valid} = \sum_{c=1}^C \chi_c(\beta) \| \left< \beta, \kappa \right> \|_2^2
\label{eq:valideshape}
\end{equation}
% 
\begin{edit}
For example to avoid middle and index fingers from permuting, one such constraint is written in the following form, and $\chi_0(\beta)=1$ only when an invalid configuration is detected:
\end{edit}
% 
\begin{equation*}
\chi_0(\beta) \| \beta_\text{idx-base-x} - \beta_\text{idx-base-rad} - \beta_\text{mid-base-x} + \beta_\text{mid-base-rad} \|_2^2
\end{equation*}

\subsection{Offline calibration}
\label{sec:batch}
\label{sec:offline}
While we focus on an online/streaming algorithm, we also describe an offline baseline calibration procedure -- inspired by the work of~\cite{taylor2014user} -- where multiple frames in the input sequence are simultaneously considered. To achieve this, \Equation{energies} is modified to consider $N$ frames, each with its own pose parameters $\theta_n$, but with the same underlying shape $\beta$, resulting in what we refer to as \emph{\OfflineHard{}} calibration:
% 
\begin{equation}
\argmin_{\beta, \{\theta_n\} } \sum_{n=1}^N \sum_{\tau \in \mathcal{T}} E_{\tau}(d_n, [\theta_n, \beta]) 
\label{eq:offlinehard}
\end{equation}
% 
Such optimization is initialized with a single $\beta^0$, and in our experiments, we noticed how this resulted in reduced convergence performance and a propensity for the optimization to fall into local minimas. 

Therefore, we introduce the \emph{\OfflineSoft{}} calibration, where the constraint that a single $\beta$ should be optimized is enforced through a soft penalty:
% 
\begin{equation}
\argmin_{\beta, \{\theta_n\} } \sum_{n=1}^N \sum_{\tau \in \mathcal{T}} E_{\tau}(d_n, [\theta_n, \beta_n]) + \omega_{\beta} \sum_{n=1}^N \| \beta_n - \beta \|^2
\label{eq:offlinesoft}
\end{equation}
%
The initializations $\beta_n^0$ is derived from the intra-frame optimization of \Equation{energies}, while the penalty weight is set to a large value ($\small \omega_{\beta}=10e4$). The advantage of \OfflineSoft{} over \OfflineHard{} can be clearly observed in \Figure{evalnyu}, where the former achieves a performance comparable to the one of the (overfitting) intra-frame optimization. Finally, note that in practice we do not consider every frame as this large problem would not fit into memory, but instead we sub-sample at a $\approx 1/20$ rate, the same subsampling is used for our online solution to avoid a bias in the comparisons. 
% \AN{This sounds like our online algorithm runs only every 20th frame. THIS IS NOT THE CASE. KF and IEKF measurements are taken every 20th frame to satisfy independent measurements assumption of the Kalman filter. The neighboring frames are not independent, they are too similar to each other.}
% \AT{see github issue }
