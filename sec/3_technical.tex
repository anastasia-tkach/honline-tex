\input{fig/intra/item}
\section{Online model calibration}
\label{sec:technical}
Given the $n$-th input depth frame $\mathcal{D}_n$ we first segment the hand point cloud via a wristband~\cite{htrack}, and represent it in vectorized form as $d_n$. With $x_n = [\theta_n; \beta_n]$ we represent vector of coalesced pose and shape parameters at frame $n$. \TODO{Here describe the hand model, or perhaps that can be done in the intro?}

To achieve online \emph{joint} calibration and tracking, our algorithm presents two fundamental components which we refer to as \emph{intra-} and \emph{inter-frame} regression. The former gathers estimates from a single frame (see \Section{independent}), while the latter integrates this knowlege across frames (see \Section{split}). We first demonstrate how inter-frame regression can be interpreted as a Kalman Filter (KF), highlight its shortcomings, and finally propose a \emph{joint} inter-/intra-frame regression and interpret it as an Iterative Extended Kalman Filter (IEKF) (see \Section{joint}).

\input{fig/inter/item}
\paragraph{Intra-frame regression}
Given a proper initialization $x_n^0$ and the input data $d_n$, intra-frame regression solves for a locally optimal $x_n^*$, as well as estimates the \emph{certainty/uncertainty} in the given solution. That is, this scheme provides a distribution for $x_n$ that is solely based on the knowledge in the depth frame $\mathcal{D}_n$. Estimating uncertainty in the solution is essential for template personalization. As we illustrate in \Figure{intra}, the confidence in regressed \emph{shape} parameters is conditional on the \emph{pose} of the current frame.
Rather than deriving this relationship via hard-coded rules, we follow the ideas in ~\todo{\cite{?}} and derive how this distribution can be approximated by a gaussian $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$.
A fundamental observation is that the parameters of this distribution can be \emph{directly} obtained from the Jacobians of the tracking energies. We illustrate why a covariance matrix $\star{\Sigma}_n$, as opposed to per-measurement variances, is necessary through the stick-figure example of \Figure{covariance}. 

\input{fig/covariance/item}
\paragraph{Inter-frame regression}
Unlike pose parameters, shape parameters are \emph{persistent} over time: we observe the same user performing in front of the camera. Further, sufficient information to estimate certain shape parameters is simply not available in certain frames. For example, by observing a straight finger like the one in \Figure{intra}, it is difficult to estimate the length of a phalanx, therefore knowledge must be gathered from a \emph{collection} of frames capturing the user in different poses. Rather than collecting a persistent model from a set of pre-recorded calibration data as in~\cite{taylor2016joint}, we approach the problem from an \emph{online-modeling} point of view. In what follows, we detail a principled method to temporally integrate parameter estimates in a way that also accounts for their confidence.

\subsection{Intra-frame regression -- $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$}
\label{sec:independent}
\label{sec:intra}
% 
Given $d_n$ the point cloud at frame $n$, and an initialization of hand pose from the previous frame $x_n^0 = x_{n - 1}^*$ we find an independent estimate of $x_n^*$ by local iterative optimization (a-la Levenberg) of the multi-objective energy function:
% 
\begin{equation}
x_n^* = \argmin_{x_n} \sum_{\tau \in \mathcal{T}} E_{\tau}(d_n, x_n) 
\label{eq:energies}
\end{equation}
% 
Where the terms $\mathcal{T}$ ensure that:
%
% \vspace{-.5\parskip}
\begin{equation*}
\setlength{\jot}{0pt}
\begin{aligned}
\text{\textbf{d2m}} & \quad \text{data points are explained by the model} \\ 
\text{\textbf{m2d}} & \quad \text{the model lies in the sensor visual-hull} \\
\text{\textbf{shape-prior}} & \quad \text{the model shape should be plausible} \\
\text{\textbf{pose-prior}} & \quad \text{the model pose should be plausible} \\
\text{\textbf{smooth}} & \quad \text{the recorded sequence is smooth} \\
\text{\textbf{collision}} & \quad \text{fingers do not interpenetrate} \\
\text{\textbf{limits}} & \quad \text{joint limits are satisfied} 
\end{aligned}
\end{equation*}
The energy terms in the objective function are detailed in~\cite{tkach2016sphere} and~\cite{htrack}, with the exception of \emph{shape-prior} that is discussed in \Section{shapeprior}.

\input{tab/kf-like}
\paragraph{Laplace approximation}
% \subsection{Posterior distribution of parameters after taking a measurement into account}
\label{sec:posterior}
While the optimization problem in \Equation{energies} estimates pose/shape parameters, information regarding the confidence of the estimate is not directly available. To retrieve this information, we first convert the \Equation{energies} in a probabilistic format, and from this formulation we then derive our uncertainties. 
% 
Towards this goal, we rewrite the \emph{d2m} and \emph{m2d} terms in \Eq{energies} as:
% 
\begin{align}
E_{\tau}(d_n, x_n) = \|I_{\tau} d_n - F_{\tau} (x_n)\|_2^2,
\end{align} 
where $I_\tau$ are identity matrices, and $I_{m2d}(i,i)=0$ if the rasterized 2D template pixel $\mathbf{p}_i$ lies within the sensor silhouette image; see \cite{htrack}. We also concatenate the two terms in pair of column vectors:
% 
\begin{align*}
F(x_n) = \left[F_{d2m}(x_n); F_{m2d}(x_n)\right] 
\:\:\text{and}\:\: 
d_n = \left[(I_{d2m} d_n); (I_{m2d} d_n) \right],
\end{align*}
% 
\AN{As we discussed last time, could you please remove this part with indicator variables since I am not using all energies anyway? It takes time to understand it and it does not add anything for the value of the paper.}
leading to a compact posterior distribution representation:
% 
\begin{align}
P(d_n|x_n) &= \exp \left( - \tfrac{1}{2}(d_n - F(x_n))^T (d_n - F(x_n)) \right)
\label{eq:posterior}
\end{align}
% 
\Equation{energies} is then rewritten in probabilistic form, where we temporarily omit the frame index $n$ for conveniency:
%
\begin{align}
>> x^* &= \argmax_{x} \underbrace{\log  P(d|x)}_{L(x)}
\label{eq:independent}
\end{align}
%
We now perform a second-order Taylor expansion of the log-likelihood of the data $L(x)$ around the \emph{optimal} solution $x^*$:
%
\begin{align}
L(x) \approx \tilde{L}(x) = L(x^*)   
+ \tfrac{\partial L(x^*) }{\partial x}  \Delta x 
+ \tfrac{1}{2} \Delta x^T\tfrac{\partial^2 L(x^*)}{{\partial x}^2} \Delta x + \text{h.o.t.}
\label{eq:taylor}
\end{align}
%
where $\Delta x=x - x^*$, and with an abuse of notation, as $\partial f(\star{x})/\partial x$ we indicate the partial derivative of $f(x)$ evaluated at $\star{x}$. Note how the Jacobian and the Hessian are respectively zero and positive definite at our optimal point $x^*$ with $\bar{F}(x_n) = d_n - F(x_n)$ for brevity of notation:
%
\begin{align}
>> \tfrac{\partial L(x^*)}{\partial x} &= - 2 \bar{F}(x^*)^T 
\tfrac{\partial \bar{F}(x^*)}{\partial x} = 0 
\label{eq:taylor-jacobian}
\\
>> \tfrac{\partial^2 L(x^*)}{\partial x^2} 
% = 2 \left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*}^T \left.\tfrac{\partial F(x)}{\partial x}\right|_{x^*} + \\ 2 F(x^*)^T  \left. \tfrac{\partial \partial F (x^*) }{\partial \partial x} \right|_{x^*}
& \approx - 2 \tfrac{\partial \bar{F}(x^*)}{\partial x}^T \tfrac{\partial \bar{F}(x^*)}{\partial x}
\triangleq %< DEFINED AS
-{\star{\Sigma}}^{-1} \succ 0
\label{eq:taylor-hessian}
\end{align}
% 
From \Equation{taylor}, remembering that $\tilde P(d|x) = \exp (\tilde{L}(x))$, we can then derive the \emph{approximated} posterior distribution:
%
\begin{align}
\tilde{P}(d|x) = \exp\left(- \tfrac{1}{2}(x - x^*)^T {\star{\Sigma}}^{-1}  (x - x^*) \right) = \mathcal{N}\left(\star{x}, \star{\Sigma} \right)
\end{align}
%
That is, after processing the information in a frame $d_n$, the sought-after quadratic approximation of posterior distribution of model parameters is a \emph{normal} distribution $\mathcal{N}\left(x_n^*, \star\Sigma_n \right)$.
Its mean is the solution of the (iterative) optimization problem in \Equation{energies}; its covariance is computed according to \Equation{taylor-hessian}, that is, from the Jacobians of the energy terms $\{E_{\text{d2m}}, E_{\text{m2d}}\}$ at the optimal solution.

\input{tab/iekf-like}
\input{fig/realtrack/item}
\subsection{Inter-frame regression -- $\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n) | \mathcal{N}(\star{x}_n, \star{\Sigma}_n)$ } 
% \subsection{Merging independent measurements}
\label{sec:split}
Given the probabilistic interpretation in \Section{independent}, a temporal regression over parameters can be obtained by cumulating the posterior distributions over the set of previous frames. This leads to the to the following pair of inductive update equations:
% 
\begin{align}
\mathcal{N}(\hat{x}_1, \hat{\Sigma}_1) &= \mathcal{N}(\star{x}_1, \star{\Sigma}_1) \\
\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n) &= \mathcal{N}(\hat{x}_{n-1}, \hat{\Sigma}_{n-1}) \mathcal{N}(\star{x}_n, \star{\Sigma}_n)
\end{align}
% 
By applying the product of Gaussians rule ~\cite{petersen2008matrix}:
% 
\begin{align}
\begin{split}
\hat{x}_{n} &= \star\Sigma_{n} (\hat{\Sigma}_{n-1} + \star\Sigma_{n})^{-1} \hat{x}_{n-1} + 
\hat{\Sigma}_{n-1} (\hat{\Sigma}_{n-1} + \star\Sigma_n)^{-1} x_n^*
\\
\hat{\Sigma}_n &= \hat{\Sigma}_{n-1} (\hat{\Sigma}_{n-1} + {\star\Sigma_n})^{-1} \star\Sigma_n
\label{eq:combining}
\end{split}
\end{align}
% 
In \Appendix{proof-kalman}, we shown how \Equation{combining} is equivalent to the Kalman Filter (KF) update equations in \Table{interframe}, with measurement $x_n^*$, and measurement noise covariance $\star\Sigma_n$. This observation is fundamental as, under the same assumptions we made in \Appendix{kalman}, \cite[\todo{Pg.?}]{maybeck1979stochastic} noted how a KF is provably \emph{optimal}.
This optimization will be used as a \emph{baseline}, as it is arguably the simplest way of achieving an online parameter regression: by treating the results of the independent solve $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$ as the measurements in a Kalman filter.

\subsection{Joint inter-frame regression}
\label{sec:joint}
% Note that the best-fitting parameters $x_n^*$ for the given the single input frame $d_n$ are computed using local LM optimization. Thus, it is crucial that optimization starts in a sensible region of parameters space.
The optimization in \Tab{interframe} does not provide any information about the current estimate of the parameters $\hat{x}_n$ to the independent solve described in \Section{independent}. This could be problematic, as in this case \Eq{energies} does not leverage any temporal information aside from initialization, while relying on a sufficiently good initialization to compute $\mathcal{N}(\star{x}_n, \star{\Sigma}_n)$. One potential way to increase the robustness of the algorithm would be to include this information in \Eq{independent}:
% 
\begin{align}
x_n^* &= \argmin_{x_n} \log  P(d_n|x_n) P(x_n |\hat{x}_{n - 1}) 
\label{eq:independenttemp}
\end{align}
% 
Instead, we propose to coalesce the inter and intra frame optimization resulting in the \emph{joint} inter-frame regression scheme in~\Table{iekf-like}. In \Appendix{ieif-lm} we demonstrate that optimizing the objective in~\Table{iekf-like} with a Levenberg-Marquardt method is equivalent to an Iterated Extended Kalman Filter (IEKF), with measurement update $d_n$.
% 
Notice how this optimization, representing our online tracking/modeling system, jointly considers the measurement $d_n$ as well as past estimates $\mathcal{N}(\hat{x}_n, \hat{\Sigma}_n)$.

\subsection{Shape prior}
\label{sec:shapeprior}
\begin{DRAFT} 
\subsubsection{Anisotropic uniform scaling shape prior}

Since a lot of hand shape variance can be explained by uniform scale, width and thickness of the hand (do you maybe know some reference?) we introduce anisotropic uniform scaling shape prior. This prior helps at first several frames if user hand size is different from the template. We add this prior with a small weight, so that it does not prevent from finding the exact parameters of the current user\rq{}s hand. The formulation is as following:

\begin{equation}
E_{uniform\_shape\_prior} = \|\beta - I\tilde{\beta} \circ \bar{\beta}\|_2^2
\end{equation}

where $\beta_{N_{\beta} \times 1}$ are the hand shape parameters, 
$\tilde\beta_{3\times1}$ are latent scale parameters - height, width and thickness, 
$I_{N_{\beta}\times 3}$ is a matrix that puts $\tilde\beta_i$ in correspondence with the relevant $\beta_j$,
$\bar{\beta}_{N_{\beta} \times 1}$ are the template hand shape parameters,
$\circ$ is element-wise product.

\subsubsection{Semantic shape prior}
Since anisotropic uniform scaling shape prior has a small weigh and thus only suggestive power, we need a prior to encode how a human hand can/cannot look like. For example, the finger base cannot be located in mid-air outside of the palm or the fingers should be in order \{index, middle, ring, pinky\} and not some other permutation of this list.
Semantic shape prior is a list of constraints each of which has a form of linear combination of $\beta_i$.

\begin{equation}
E_{semantic\_shape\_prior} = \sum_{c = 1}^{N} \chi_c  {\sum_{i = 1}^{N_{c}} \kappa_{c_i} \beta_{c_i}}
\end{equation}

with $\chi_c = 1$ when semantic constrain $c$ is violated and $\chi_c = 0$ otherwise.

where $\kappa_{c_i}$ are scalar coefficients.
\end{DRAFT}

\subsection{Something else?}
\AT{Leaving some space: i am sure we are forgetting something in technical, like the fact that you don't run honline at each frame because it's too heavy?}
\begin{DRAFT}
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.  Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.  Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. 
\end{DRAFT}

\subsection{Batch Offline/Online}
\label{sec:batch}
\TODO{ANDREA}

\input{fig/interreal/item}
