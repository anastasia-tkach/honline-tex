\section{Related Works}
Due to the vast amount of work in the area of body, face and hand tracking we respectively refer the reader to the recent works of \cite{bogo2015detailed}, \cite{cao2016real} and \cite{taylor2016joint} for a complete overview, while in this section we focus our attention on generative hand tracking and model calibration. Model personalization is a core ingredient in generative motion tracking \cite{pons2011model} and in this section we will focus on hand model calibration, keeping in mind that due to the large number of hand self occlusions, the amount of signal-to-noise ratio in current depth sensors, a globally unconstrained pose, and the similar appearance of fingers make  the personalization of a hand model is a significantly harder problem than face or body model calibration; see~\cite{supancic2015depth} for a detailed discussion.
% hands are highly articulated, their global pose is unconstrained, and fingers do not only have the same appearance, but they also cause frequent occlusions and self-occlusions
% \MP{For the related work, we might briefly discuss why hand calibration is significantly different (and harder) than face calibration, to avoid the impression that one could just use the same methods}

\paragraph{Off-line model calibration}
\cite{albrecht2003construction} pioneered the construction of realistic (skin, bone and muscles) personalized models proposing a pipeline for the registration of a 3D mesh model to RGB data manually pre-processed by the user. Reducing the amount of manual interaction required from the user, \cite{rhee2006human} showed how skin creases and silhouette images can also be used to guide the registration of a model to color imagery. \cite{taylor2014user} introduced a more automatic pipeline, generating personalized hand models from input depth sequences where the user rotates his hand while articulating fingers. More closely related to ours is the work by \cite{tan2016fits}, demonstrating how to robustly personalize a hand model to an individual user from a set of depth measurements using trained shape basis such the one proposed by~\cite{khamis2015learning}. The calibration pipeline, although robust and efficient, is not fully automated as the user needs to manually pick the set of frames over which the calibration optimization is performed. 
In facial calibration~\cite{weise2011realtime}, users are asked to assume a set of standard facial expressions to match standard poses in the Facial Action Coding System (FACS)~\cite{facs}.
Inspired by these approaches, \cite{taylor2016joint} recently proposed an analogous offline hand calibration session, but the set of \emph{standard} hand poses that allow to properly capture a complete view hand geometry has yet to be addressed by researchers. Hence, none of the above methods is suitable or easily adaptable to the kind of consumer-level applications that we target, \todo{being the absence of complex manual calibration or extensive pre-processing mandatory requirements for our framework.} \AT{see github}

\paragraph{On-line model calibration}
In \cite{delagorce2011model}, the authors introduced a model-based approach for hand tracking from monocular RGB video sequence. Hand pose, texture and lighting are dynamically estimated through minimization of an objective function, while shape is determined by optimizing its parameters over the first frame only. 
Recently \cite{makris2015model} proposed a model based approach to jointly solve the pose tracking and shape estimation from depth measurements in an online framework, solving for the cylindrical geometry of a hand through render-and-compare evaluations over a set of frames optimized by particle swarm optimization. The pipeline they developed runs in real-time (30fps), but lacks the degree of robustness and accuracy desirable for consumer level applications. 
More sophisticated approaches to information agglomeration such as the one by \cite{bouaziz2013online}, where shape estimation is carried over the whole set of frames aggregated with exponential decay, allow to obtain more accurate results, while guaranteeing real-time performances. 
Interestingly \cite{zou2013coslam} designed a pipeline for SLAM in dynamic environments employing a time aggregation technique similar to ours. Map 3D point positions are maintained together with their associate uncertainty through a co-variance matrix, and are refined whenever a new observation is available.

% \paragraph{Appearance-based model calibration}
% We might have to add this if needed