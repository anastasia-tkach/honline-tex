\section{Technical}
Given the hand point cloud at the first frame $d_1$, we find the best-fitting parameters, and then derive a quadratic approximation of their posterior distribution $\mathcal{N}(\hat x_1, \hat \Sigma_1)$; see Section \ref{sec:independent-solve}.
\TODO{describe how to combine}

\subsection{Intra-frame regression \todo{(independent)}}
\label{sec:independent-solve}
% 
Denote the vector of all hand parameters as $x_n = [\theta_n; \beta_n]$ and the segmented hand point cloud in vectorized form as $d_n$. Given $d_n$ and initialization of hand pose from previous frame $x_n^0 = x_{n - 1}^*$ we find an independent estimate of $x_n^*$ by local Gauss-Newton optimization of the multi-objective energy function:
% 
\begin{equation}
x_n^* = \argmin_{x_n} \sum_{\tau \in \mathcal{T}} E_{\tau}(d_n, x_n) \label{eq:energies}
\end{equation}
% 
Where $\mathcal{T}$ is the following set:
%
\vspace{-.5\parskip}
\begin{equation*}
\setlength{\jot}{0pt}
\begin{aligned}
\text{\textbf{d2m}} & \quad \text{data points are explained by the model} \\ 
\text{\textbf{m2d}} & \quad \text{the model lies in the sensor visual-hull} \\
\text{\textbf{shape-prior}} & \quad \text{model shape should be plausible} \\
\text{\textbf{pose-prior}} & \quad \text{model pose should be plausible} \\
\text{\textbf{limits}} & \quad \text{joint limits should hold} \\
\text{\textbf{collision}} & \quad \text{fingers should not interpenetrate} 
\end{aligned}
\end{equation*}
With the exception of \emph{shape-prior}, \todo{see \Section{shapeprior}}, the energy terms in the objective function are defined in~\cite{tkach2016sphere}.

\paragraph{Probabilistic interpretation of least-squares}
\begin{DRAFT}
All the energy term $E_{\tau}(d_n, x_n)$ can be rewritten in a form $\|I_{\tau} d_n - F_{\tau} (x_n)\|_2^2$ with $I_{d2m}$ an identity matrix, $I_{m2d}$ a permutation(/reduction?) matrix that removes from $d_n$ the entries that are not the closest for any model point and the remaining $I_{\tau}$ all zeros matrices. 
Denote $\left[F_{d2m}^T(x_n), ..., F_{pose}^T(x_n)\right]^T$ as $F(x_n)$ and overload $d_n$ as  $\left[(I_{d2m} d_n)^T, ..., (I_{pose} d_n)^T\right]^T$  for brevity of notation.
\end{DRAFT}
% 
\begin{align}
x^*_1 &= \argmin_{x_1} \underbrace{\log  P(d_1|x_1)}_{L(x_1)} 
\\
P(d_1|x_1) &= \mathcal{N}(F(x_1), I) 
% \exp \left( - (d_1 - F(x_1))^T (d_1 - F(x_1)) \right)
\label{eq:independent}
\end{align}
% 

\paragraph{Posterior quadratic approximation} 
\label{sec:posterior}
% 
Following~\todo{[??]}, we expand the log-likelihood of the data $L(x_1)$ around the optimal solution $x_1^*$ to the second order:
% 
\begin{align}
L(x_1) \approx L(x_1^*)   
+ J_{x_1} \Delta x
+ \tfrac{1}{2} \Delta x^T H_{x_1} \Delta x 
+ \text{h.o.t.}
\end{align}
%
where $\Delta x=x_1 - x_1^*$ and Jacobian and Hessian are:
\begin{align*}
J_{x_1} &= 
\left.\tfrac{\partial L}{\partial x_1} \right|_{x_1^*} = - 2 F(x_1^*)^T + 2 F(x_1)^T 
\left.\tfrac{\partial F}{\partial x_1}\right|_{x_1^*} = 0
\\
H_{x_1} &= 
\left.\tfrac{\partial^2 L}{\partial x_1^2}\right|_{x_1^*} 
\approx 2
\left.\tfrac{\partial F}{\partial x_1}\right|_{x_1^*}^T  
\left.\tfrac{\partial F}{\partial x_1}\right|_{x_1^*}
% + 2 F(x_1^*)^T \left.\frac{\partial^2 F}{\partial x_1^2}\right|_{x_1^*}
\succ 0
\end{align*}
% 
Converting the likelihood back into a probability form, we can therefore see that our posterior follows a normal distribution:
% 
\begin{equation}
P(d_1|x_1) 
\propto \mathcal{N}(x_1^*, H_{x_1}^{-1}) 
= \mathcal{N}(\hat x_1, \hat \Sigma_1)
% \exp\left(- 0.5(x_1 - x_1^*)^T \Sigma_1^{-1}  (x_1 - x_1^*) \right)\\
\end{equation}

\subsection{Inter-frame regression \todo{(combining)}} 
\label{sec:combining}
\AN{The shape prior should only be used in the first frame, because afterwards it will be incorporated inside of Kalman prior}.

Given the second frame $d_2$, we compute the corresponding value of the parameters $x_2^*$ and their variance $\Sigma_2$ the same way as in \ref{sec:posterior}. The posterior distribution of the parameters now has the form 

\begin{equation}
\mathcal{N}(\hat{x}_2, \hat{\Sigma}_2) = \mathcal{N}(\hat{x}_1, \hat{\Sigma}_1) \mathcal{N}(x_2^*, \Sigma_2)
\end{equation}

Applying the product of Gaussians rule presented in \cite{petersen2008matrix}, we get 
\begin{align}
\begin{split}
\hat{x}_2 = \Sigma_2 (\hat{\Sigma}_1 + \Sigma_2)^{-1} \hat{x}_1 + 
\hat{\Sigma}_1 (\hat{\Sigma}_1 + \Sigma_2)^{-1} x_2^*\\
\hat{\Sigma}_2 = \hat{\Sigma}_1 (\hat{\Sigma}_1 + \Sigma_2)^{-1} \Sigma_2
\end{split} \label{eq:gaussians-product}
\end{align}

In Appendix \ref{app:kalman} it is shown that the Equations \ref{eq:gaussians-product} are equivalent to Kalman filter update with measurement $x_n^*$ and measurement noise covariance $\Sigma_n$. This is important, because, according to P. Maybeck \cite{maybeck1979stochastic}, under the assumptions presented in Appendix \ref{app:kalman} ''... the Kalman filter can be shown to be the best filter of any conceivable form. It incorporates all information that can be provided to it. ''

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!h] 
\centering
\caption{KF: Combining independent measurements\label{tab:kf-like}} 
\begin{tabular}{|c|}
\hline
$x_n^* = \argmax_{x_n} \log P(d_n|x_n)$ \\
$\hat{x}_n = \operatorname{argmax}_{x_n} \underbrace{\log \left( P(x_n|x_n^*) P(x_n |\hat{x}_{n - 1}) \right)}_{L(x_n)}$\\
with \\
$P(x_n |x_n^*) = \exp \left( - (x_n - x_n^* )^T \Sigma_n^{-1}(x_n - x_n^* )\right)$ \\
$P(x_n |\hat{x}_{n - 1}) = \exp \left( - (x_n - \hat{x}_{n - 1} )^T \hat{\Sigma}_{n - 1}^{-1} (x_n - \hat{x}_{n - 1} )\right)$ \\	

$\hat{\Sigma}_n^{-1} = \frac{\partial \partial L}{\partial x_n \partial x_n}(\hat{x}_n) \approx $ \\
\\
$\left[
	\begin{array}{cc}
		\left(\Sigma_n^{-1}\right)^{1/2} \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right]^T 
\left[
	\begin{array}{c}
		\left(\Sigma_n^{-1}\right)^{1/2} \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right] = \hat{\Sigma}_{n-1}^{-1}  + \Sigma_{n}^{-1}$ \\

\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Regularizing optimization}

Note that the best-fitting parameters $x_n^*$ for the given the single input frame $d_n$ are computed using local LM optimization. Thus, it is crucial that optimization starts in a sensible region of parameters space. The optimization presented in Table \ref{tab:kf-like} does not provided any information about the current estimate of the parameters $\hat{x}_n$ to the independent solve from Section \ref{sec:independent-solve}. One way to remedy this is include the term $P(x_n |\hat{x}_{n - 1})$ directly in the independent solve.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!h] 
\centering
\caption{Objective function of IEKF \label{tab:iekf-like}} 
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|c|}

\hline

$\hat{x}_n = \operatorname{argmax}_{x_n} 
\underbrace{\log \left( P(d_n|x_n) P(x_n |\hat{x}_{n - 1}) \right)}_{L(x_n)}$\\ 
with \\
$P(d_n|x_n) = \exp \left( - (d_n - F(x_n))^T (d_n - F(x_n)) \right)$ \\
$P(x_n |\hat{x}_{n - 1}) = \exp \left( -(x_n - \hat{x}_{n - 1})^T \hat{\Sigma}_{n - 1}^{-1} (x_n - \hat{x}_{n - 1})\right)$ \\
\\
$\hat{\Sigma}_n^{-1} = \frac{\partial \partial L}{\partial x_n \partial x_n}(\hat{x}_n) \approx $ \\
\\
$\left[
	\begin{array}{cc}
		- \frac{\partial F} {\partial x_n} (\hat{x}_n) \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right]^T 
\left[
	\begin{array}{c}
		- \frac{\partial F} {\partial x_n} (\hat{x}_n) \\
		\left(\hat{\Sigma}_{n - 1}^{-1}\right)^{1/2} \\
	\end{array}
\right] = $\\
\\
$ = \hat{\Sigma}_{n-1}^{-1} + \frac{\partial F} {\partial x_n} (\hat{x}_n)^T \frac{\partial F} {\partial x_n} (\hat{x}_n) = \hat{\Sigma}_{n-1}^{-1}  + \Sigma_{n}^{-1}$	\\
\hline
\end{tabular}
}
\end{table}

In Appendix \ref{app:ieif-lm} we demonstrate that optimizing the objective function presented in Table \ref{tab:iekf-like} using Levenberg-Marquardt algorithm is equivalent to measurement update of Iterated Extended Kalman Filter.