\section{Introduction}
% AR/VR
In our everyday life, we interact with the surrounding environment using our hands. A main focus of recent research has been to bring such interaction form to virtual objects, such as the one projected in virtual reality devices (Oculus Rift, HTC Vive), or super-imposed as a hologram in AR/MR headsets (Microsoft Hololens, MagicLeap, Intel Alloy). 
% pre-viz 
Performance capture is also essential in film and game production for pre-visualization, where motion can be transferred in real-time to a virtual avatar. This allows directors to plan shots more effectively, reduce turn-around times and hence costs.
% requirement: seamless integration
For these applications, it is desirable for the tracking technology to be robust, accurate, and have a \emph{seamless deployment}, since performance capture can happen at an animator's desk, on a movie set, or even \emph{in-the-wild} where the user might not be aware of its operative requirements (e.g. advertisement or security).

% typical structure of tracking pipeline
\paragraph{Hand tracking from monocular depth}
Recent developments in hand motion capture technology have brought us a step closer in achieving effective tracking, where hardware solutions such as data-gloves, reflective markers and multi-camera setups have been shelved due to their invasiveness, as well as to the cumbersomeness in their setup.
% color vs. depth
Hence, a single camera has become the standard acquisition device, where depth cameras (e.g. Intel RealSense SR300) have taken a solid lead over color imagery to overcome the many challenges of hand-tracking~\cite{supancic2015depth}. 
% discriminative + generative
Modern techniques (e.g. \cite{taylor2016joint}) often rely on \emph{discriminative} techniques~(e.g. \cite{valentin2016learning}) to identify a coarse pose, followed by a \emph{generative} stage~(e.g. \cite{tkach2016sphere}) to refine the alignment and obtain a precise pose estimate.


% template + personalization
\paragraph{Tracking templates and personalization}
% generic calibration
Depth imagery provides an incomplete 3D scansion of the tracked object, and generative trackers attempt to register a \emph{template} to 3D data so to minimize alignement residuals. The more accurate a template is to the observed user, the better tracking accuracy can be achieved~\cite{tkach2016sphere,taylor2016joint}. The process of accurately generating a template from input data is referred to in the literature as \emph{calibration} or \emph{personalization}. 
% calibration methods -> mostly offline
Calibrating a template from a set of static poses is a standard component in the workflow of facial performance capture~\cite{weise2011realtime,thabo}, and the work of  \cite{taylor2014user} pioneered it within the realm of hand tracking. However, current methods such as \cite{taylor2016joint} and \cite{tkach2016sphere} suffer a major drawback: the template must be created during a controlled calibration stage, where the hand is scanned in several static poses~(i.e. \emph{offline}). While appropriate for professional use, a calibration session is a severe drawback for seamless deployment in consumer-level applications.
% online calibration
Therefore, inspired by recent efforts in facial performance capture that calibrate templates while tracking~\cite{li_sig13,bouaziz2013online}, in this paper we propose a pipeline for real-time \emph{online} model calibration. The approach we present has been tailored to monocular hand tracking, where we tackle the significant technical challenges created by missing data due to self-occlusions.

\paragraph{Contributions}
Our core contribution is a principled way to integrate per-frame information into an online real-time pose/shape tracking algorithm: one that estimates the hand's \emph{pose}, while simultaneously refining the \emph{shape} of the user being tracked. That is, as more of the user model and articulation is observed during tracking, the more the tracking template is progressively adapted to match the performer, which in turns results in more accurate motion tracking. Due to self-occlusions, only a small subset of degrees of freedom can be inferred from a single frame. Our technique automatically discovers the \emph{confidence} in the per-frame parameter estimates, and leverages this information to build a tracking model that selectively \emph{accumulate} confident estimates over time. The validity of our technical solution is corroborated by re-interpreting our algorithm as an optimal Kalman filter, and its performance evaluated in comparison to state-of-the-art algorithms. Overall, our solution yields a fully automatic, real-time hand tracking system that is well-suited for consumer applications.
